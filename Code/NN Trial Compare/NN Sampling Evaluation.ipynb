{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing Search Algorithms - ML-DO Models\n",
    "#Disease Biophysics Group\n",
    "#Written by John Zimmerman\n",
    "#Updated 9/12/22\n",
    "\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "#from scipy import interpolate\n",
    "import scipy.interpolate\n",
    "import scipy.stats\n",
    "import math\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "import h5py\n",
    "import GeoSwimmer\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.compat.v2.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateModelLabels(df,model,chunksize=4):\n",
    "    splits = int(np.round(df.DNA.size/chunksize))\n",
    "    breakList = list(SwimNN.SwimSearch.chunks(np.arange(df.DNA.size),splits))\n",
    "    modelLabel = np.zeros(df.DNA.size)\n",
    "    print('Updaing Labels')\n",
    "    for breaks in breakList:\n",
    "        print(f'{(breaks[-1]/df.DNA.size)*100:.2f}%...')\n",
    "        dfchunk = df[breaks[0]:breaks[-1]+1]\n",
    "        OH_df = SwimNN.NN.OneHotEncodeList(dfchunk)\n",
    "        modelLabel[breaks[0]:breaks[-1]+1] = model(OH_df,training=False).numpy().reshape(-1)\n",
    "        #modelLabel[breaks[0]:breaks[-1]+1] = model(OH_df[breaks[0]:breaks[-1]+1],training=False).numpy().reshape(-1)\n",
    "\n",
    "    return modelLabel\n",
    "\n",
    "def GenFooDataframe(dnaLength):\n",
    "    #---Make Swimmer Dataframe with Foo Velocity Labels ---\n",
    "    #Takes ~15 min to run\n",
    "    print('Generating Foo dataframe. Takes ~15 min')\n",
    "    #Generate List of all possible swimmers\n",
    "    dnaAlphabet = SwimDNA.DNABasis()\n",
    "    fullswimlist = [''.join(i) for i in product(dnaAlphabet, repeat = dnaLength)]\n",
    "    fullswimlist = fullswimlist[1:-1]\n",
    "    df = pd.DataFrame({'DNA':fullswimlist})\n",
    "\n",
    "    #SampleSwim = Swimlist.sample(100000,random_state=seed)\n",
    "    print('Label Velocities...')\n",
    "    dfLabels = df.DNA.apply(SwimVel.gen_approx_swim_vels, args=(.5,0.0,0.0,0,.05)) \n",
    "    print('Done')\n",
    "\n",
    "    alphabetVectors = RadarPlot.constructRadarBasis(SwimDNA.DNABasis())\n",
    "    print('Radar Points...')\n",
    "    RadarPoints = df.DNA.apply(RadarPlot.DNARadarPointsList, args=(alphabetVectors,))\n",
    "    RadarPoints = pd.DataFrame(RadarPoints.to_list(),columns=['RadX','RadY'])\n",
    "    print('Done')\n",
    "\n",
    "    df['RadX']=RadarPoints.RadX.to_list()\n",
    "    df['RadY']=RadarPoints.RadY.to_list()\n",
    "    df['Label'] = dfLabels\n",
    "    return df\n",
    "\n",
    "class SwimVel:\n",
    "    def gen_approx_swim_vels(DNA,syn1mag=0.5,syn2mag=0.5,syn3mag=0.5,syn4mag=0.5,syn5mag=0.25):\n",
    "        swimVels = SwimVel.approx_dna_to_vel_base(DNA)+SwimVel.approx_dna_to_vel_syn5(DNA,syn5mag)+SwimVel.approx_dna_to_vel_syn1(DNA,syn1mag)+SwimVel.approx_dna_to_vel_syn2(DNA,syn2mag)+SwimVel.approx_dna_to_vel_syn3(DNA,syn3mag)+SwimVel.approx_dna_to_vel_syn4(DNA,syn4mag)\n",
    "        return swimVels\n",
    "    \n",
    "    def approx_dna_to_vel_base(DNA):\n",
    "        val = 0\n",
    "        weights = SwimDNA.getWeights(DNA)\n",
    "        for a,wgt in zip(DNA,weights):\n",
    "            val = val + np.sin((float(ord(a)-65)/13.)*4*np.pi+np.pi*5/4)*wgt\n",
    "        return val\n",
    "    \n",
    "    def approx_dna_to_vel_syn1(DNA, mag=0.5):\n",
    "        #adds value for streaks (JKLMN)\n",
    "        val = 0\n",
    "        if mag !=0:\n",
    "            weights = SwimDNA.getWeights(DNA)\n",
    "            i=0\n",
    "            for a in DNA:\n",
    "                if i>0:\n",
    "                    if (ord(a)-ord(DNA[i-1])!=0):\n",
    "                        val = val + ((13-np.abs(ord(a)-ord(DNA[i-1])))/13)*weights[i]\n",
    "                        #val = val + mag*(1-weights[i])\n",
    "                i+=1\n",
    "        return val*mag\n",
    "\n",
    "    def approx_dna_to_vel_syn2(DNA, mag=0.5):\n",
    "        #Subtracts value for negative streaks (NMLKJ)\n",
    "        val = 0\n",
    "        if mag !=0:\n",
    "            weights = SwimDNA.getWeights(DNA)\n",
    "            i=0\n",
    "            for a in DNA:\n",
    "                if i>0:\n",
    "                    if (ord(a)-ord(DNA[i-1])==-1):\n",
    "                        val = val - mag*weights[i]\n",
    "                i+=1\n",
    "        return val\n",
    "    \n",
    "    def approx_dna_to_vel_syn3(DNA, mag=0.5):\n",
    "        #Multiples Weights from first half with second half - rev order\n",
    "        weights = SwimDNA.getWeights(DNA)\n",
    "        chr_values = np.zeros(len(DNA))\n",
    "        if mag !=0:\n",
    "            i = 0\n",
    "            for a,wgt in zip(DNA,weights):\n",
    "                chr_values[i] =  ((13-float(ord(a)-65))/13.)*wgt\n",
    "                i+=1\n",
    "        \n",
    "        return np.sin(chr_values.reshape(2,-1).prod(axis=0)*10).sum()*mag    \n",
    "    \n",
    "    def approx_dna_to_vel_syn4(DNA, mag=0.5):\n",
    "        #Adds value for middle charcter interactions\n",
    "        weights = SwimDNA.getWeights(DNA)\n",
    "        characters = np.zeros(len(DNA))\n",
    "        if mag !=0:\n",
    "            i=0\n",
    "            for a, w in zip(DNA, weights):\n",
    "                characters[i] = float(ord(a) - 65)\n",
    "                i += 1\n",
    "            weights[:2] = 0\n",
    "            weights[-2:] = 0\n",
    "        return np.dot(characters, weights).sum()*mag\n",
    "\n",
    "    def approx_dna_to_vel_syn5(DNA, mag=0.25):\n",
    "        #Random depending on repeat charcters\n",
    "        counts = {a:0 for a in SwimDNA.DNABasis()}\n",
    "        val = 1\n",
    "        if mag !=0:\n",
    "            for a in DNA:\n",
    "                counts[a]+=1\n",
    "            for k,v in counts.items():\n",
    "                if (v == 1):\n",
    "                    val +=1\n",
    "                elif (v > 1):\n",
    "                    val -= v/5\n",
    "        return val*mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwimDNA:\n",
    "    def getWeights(DNA):\n",
    "        weights = np.ones(len(DNA))\n",
    "        m = -1/float(len(DNA))\n",
    "        b = 1\n",
    "        for i in range(0,len(DNA)):\n",
    "            weights[i] = m*i+b\n",
    "        weights = weights/np.sum(weights)\n",
    "        return weights\n",
    "\n",
    "    def string_to_matrix(string):\n",
    "        matrix = np.zeros((14,len(string)), dtype=np.int8)\n",
    "        for position, letter in enumerate(string):\n",
    "            num = (ord(letter.upper()) - 65) % 14\n",
    "            matrix[num, position] = 1\n",
    "        return matrix\n",
    "\n",
    "    \n",
    "    def DNABasis():\n",
    "        return ['A','B','C','D','E','F','G','H','I','J','K','L','M','N']\n",
    "    \n",
    "    def avgDnaListDistance(dnalist):\n",
    "        #Feed in dataframe.DNA list\n",
    "        avglist = np.zeros(dnalist.size) #Declare List for storing Averages\n",
    "\n",
    "        for DNA1,i in zip(dnalist,np.arange(dnalist.size)): #Cycled through each DNA\n",
    "            dist = 0\n",
    "            for DNA2 in dnalist: #Compare to Each Other DNA in database\n",
    "                if DNA1 != DNA2:\n",
    "                    dist += SwimDNA.dnaDistance(DNA1,DNA2)\n",
    "            dist = dist/float(dnalist.size-1) #Average Out Values\n",
    "            avglist[i] = dist\n",
    "        return avglist\n",
    "\n",
    "    def dnaDistance(DNA1,DNA2):\n",
    "        vector1 = SwimDNA.DNAVector(DNA1)\n",
    "        vector2 = SwimDNA.DNAVector(DNA2)\n",
    "        dist = 0\n",
    "        for a in SwimDNA.DNABasis():\n",
    "            dist += (vector1[a]-vector2[a])**2\n",
    "\n",
    "        return np.sqrt(dist)\n",
    "\n",
    "    def DNAVector(DNA):\n",
    "        weights = SwimDNA.getWeights(DNA)\n",
    "        vector = {a:0 for a in SwimDNA.DNABasis()}\n",
    "        num = np.arange(len(DNA))\n",
    "\n",
    "        for a,i in zip(DNA,num):\n",
    "                    vector[a]+=1*weights[i]\n",
    "        return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadarPlot:\n",
    "    #Radar Plot\n",
    "    def constructRadarBasis(dnaAlphabet):\n",
    "        ang = np.linspace(0,2*np.pi,len(dnaAlphabet)+1)\n",
    "        alphabetVectors = np.zeros((len(dnaAlphabet),2))\n",
    "\n",
    "        i = 0\n",
    "        for theta in ang[:-1]:\n",
    "            alphabetVectors[i,0],alphabetVectors[i,1]=np.sin(theta),np.cos(theta)\n",
    "            i+=1\n",
    "        return alphabetVectors\n",
    "\n",
    "    def DNARadarPoints(DNA,alphabetVectors):\n",
    "        weights = SwimDNA.getWeights(DNA)\n",
    "        mDNA = SwimDNA.string_to_matrix(DNA)\n",
    "        x = np.sum(weights*mDNA.T.dot(alphabetVectors[:,0]))\n",
    "        y = np.sum(weights*mDNA.T.dot(alphabetVectors[:,1]))\n",
    "        return x,y\n",
    "    \n",
    "    def DNARadarPointsList(DNA,alphabetVectors):\n",
    "        weights = SwimDNA.getWeights(DNA)\n",
    "        mDNA = SwimDNA.string_to_matrix(DNA)\n",
    "        x = np.sum(weights*mDNA.T.dot(alphabetVectors[:,0]))\n",
    "        y = np.sum(weights*mDNA.T.dot(alphabetVectors[:,1]))\n",
    "        return np.array([x,y])\n",
    "    \n",
    "    \n",
    "    def SwimListRadarPoints(SwimList):\n",
    "        alphabetVectors = RadarPlot.constructRadarBasis(SwimDNA.DNABasis())\n",
    "        xlist = np.zeros(len(SwimList.DNA))\n",
    "        ylist = np.zeros(len(SwimList.DNA))\n",
    "        i=0\n",
    "        for DNA in SwimList.DNA:\n",
    "            xlist[i],ylist[i] = RadarPlot.DNARadarPoints(DNA,alphabetVectors)\n",
    "            i+=1\n",
    "        return xlist,ylist \n",
    "    \n",
    "    def PlotRadarMesh(xdata,ydata,zdata,res=100,figheight=5,figwidth=5,vmin=0,vmax=1):\n",
    "        alphabetVectors = RadarPlot.constructRadarBasis(SwimDNA.DNABasis())\n",
    "        \n",
    "        x = np.linspace(-1,1,res)\n",
    "        y = np.linspace(-1,1,res)\n",
    "        xs,ys = np.meshgrid(x,y)\n",
    "\n",
    "        gridinterp = scipy.interpolate.griddata((xdata,ydata),zdata,(xs,ys))\n",
    "\n",
    "\n",
    "        #Plot Velocity Map\n",
    "        fig, ax = subplots(figsize=(figwidth,figheight))\n",
    "        cms = matplotlib.cm.jet\n",
    "        normv = matplotlib.colors.Normalize()\n",
    "        normv.autoscale(np.array([vmin,vmax]))\n",
    "        vsm = matplotlib.cm.ScalarMappable(cmap=cms,norm=normv)\n",
    "        vsm.set_array([])\n",
    "        #cms = matplotlib.cm.jet\n",
    "\n",
    "\n",
    "        ax.pcolormesh(xs,ys, gridinterp,cmap=cms,vmin=vmin,vmax=vmax)\n",
    "        \n",
    "        \n",
    "        circle = Circle((0, 0), 1, fill=False,lw=2,ls='--')\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        #Radar Labels\n",
    "        i=0\n",
    "        for letter in SwimDNA.DNABasis():\n",
    "            text(alphabetVectors[i,0]*1.1,alphabetVectors[i,1]*1.1,letter,fontweight='bold')\n",
    "            i+=1\n",
    "        \n",
    "        cbar = fig.colorbar(vsm)\n",
    "        cbar.set_label(('Velocity'), rotation=270,fontsize=30,labelpad=30)\n",
    "        \n",
    "        axis('off')\n",
    "        axis('scaled')\n",
    "        #fig.tight_layout(rect=[-1.5, 1.5, -1.5, 1.5])\n",
    "        return fig\n",
    "    \n",
    "    def EvenGrid(circres,radres):\n",
    "        if radres<1:\n",
    "            radres = 1\n",
    "        if circres<1:\n",
    "            circres = 1\n",
    "        \n",
    "        ang = np.linspace(0,2*np.pi,len(dnaAlphabet)*int(np.round(circres)))\n",
    "        line = np.linspace(0.1,1,int(np.round(radres))+1)\n",
    "        #line = 1-(1-line)**2\n",
    "        line = line/line.max()\n",
    "        \n",
    "        x = np.zeros(ang.size*line.size+1)\n",
    "        y = np.zeros(ang.size*line.size+1)\n",
    "        #print(f'xsize: {x.size}')\n",
    "        #print(f'ysize: {y.size}')\n",
    "        \n",
    "        i=0\n",
    "        for theta in ang:\n",
    "            for point in line:\n",
    "                x[i],y[i]=np.sin(theta)*point,np.cos(theta)*point\n",
    "                #print(point)\n",
    "                i+=1\n",
    "            #print(i)\n",
    "        return x,y    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "\n",
    "    def OneHotEncodeList (SwimList):\n",
    "        trainMatrix  =np.zeros((SwimList.DNA.size,14,len(SwimList.DNA.iloc[0])))\n",
    "\n",
    "        i=0\n",
    "        for DNA in SwimList.DNA:\n",
    "            trainMatrix[i,:,:] = SwimDNA.string_to_matrix(DNA)\n",
    "            i+=1\n",
    "\n",
    "        trainMatrix= tf.convert_to_tensor(trainMatrix,dtype=tf.float16)    \n",
    "        return trainMatrix\n",
    "\n",
    "    def LabelTensor (SwimList):\n",
    "        labelMatrix  =np.zeros(SwimList.Label.size)\n",
    "\n",
    "        i=0\n",
    "        for DNA in SwimList.DNA:\n",
    "            trainMatrix[i,:,:] = SwimDNA.string_to_matrix(DNA)\n",
    "            i+=1\n",
    "\n",
    "        trainMatrix= tf.convert_to_tensor(trainMatrix,dtype=tf.float32)    \n",
    "        return trainMatrix\n",
    "\n",
    "    def genNNModel(InputLength):\n",
    "        model = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Flatten(input_shape=(14, InputLength)),\n",
    "          tf.keras.layers.Dense(14, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.2),\n",
    "          tf.keras.layers.Dense(32, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.2),\n",
    "          tf.keras.layers.Dense(14, activation='relu'),\n",
    "          tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "   \n",
    "    def genNN_MatMul_model(InputLength):# define two sets of inputs\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "\n",
    "        # Dense Input Branch - Make a 14 dim vector\n",
    "        x = tf.keras.layers.Flatten()(inputA)\n",
    "        x = tf.keras.layers.Dense(14, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Reshape((14, 1))(x)\n",
    "        #x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "        \n",
    "        #Build cross-correlation Matrix\n",
    "        y = tf.keras.layers.Flatten()(inputA)\n",
    "        y = tf.keras.layers.Dense(14*14, activation=\"relu\")(y)\n",
    "        y = tf.keras.layers.Reshape((14, 14))(y)\n",
    "        #x = tf.keras.layers.BatchNormalization()(x)\n",
    "        y = tf.keras.Model(inputs=inputA, outputs=y)\n",
    "\n",
    "        \n",
    "        #combine the output of the two branches\n",
    "        combined = tf.linalg.matmul(y.output, x.output)\n",
    "\n",
    "        # Combined Outputs\n",
    "        z = tf.keras.layers.Flatten()(combined)\n",
    "        z = tf.keras.layers.Dense(14, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dropout(0.2)(z)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=z)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def Hessian_DualLoss_refine(InputLength):\n",
    "        #Inputs\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "        \n",
    "        #Baseline Importance of Each Function\n",
    "        add_branch = tf.keras.layers.Flatten()(inputA)\n",
    "        add_branch = tf.keras.layers.LeakyReLU()(add_branch)\n",
    "        add_branch = tf.keras.layers.Dense(6, activation=\"relu\")(add_branch)\n",
    "        add_branch = tf.keras.Model(inputs=inputA, outputs=add_branch)\n",
    "        \n",
    "        \n",
    "        #Branch 2 - Interactions\n",
    "        hessian = tf.keras.layers.Flatten()(inputA)\n",
    "        hessian = tf.keras.layers.Dense(14*14, activation=\"relu\",use_bias=False)(hessian)\n",
    "        y = tf.keras.layers.Reshape((-1, 14, 14))(hessian)\n",
    "        y = tf.keras.layers.LeakyReLU()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=True)(y)\n",
    "        y = tf.keras.layers.Dropout(0.15)(y)\n",
    "        y = tf.keras.layers.LeakyReLU()(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=True)(y)\n",
    "        y = tf.keras.layers.Dropout(0.15)(y)\n",
    "        y = tf.keras.layers.LeakyReLU()(y)\n",
    "        y = tf.keras.layers.Conv2D(2, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Dropout(0.15)(y)\n",
    "        y = tf.keras.layers.LeakyReLU()(y)\n",
    "        y = tf.keras.layers.Conv2D(6, (4, 4), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Dropout(0.15)(y)\n",
    "        y = tf.keras.layers.LeakyReLU()(y)        \n",
    "        y = tf.keras.layers.GlobalMaxPooling2D()(y)\n",
    "        y = tf.keras.layers.Dense(6, activation=\"relu\")(y)\n",
    "        y = tf.keras.Model(inputs=inputA, outputs=y)\n",
    "\n",
    "        #combine the output of the two branches - add branch gives main weight, multiply gives interaction\n",
    "        combined = tf.keras.layers.concatenate([add_branch.output,y.output])\n",
    "\n",
    "        # Combined Outputs\n",
    "        z = tf.keras.layers.Flatten()(combined)\n",
    "        z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "        \n",
    "        base = tf.keras.layers.Flatten()(add_branch.output)\n",
    "        base = tf.keras.layers.LeakyReLU()(base) \n",
    "        base = tf.keras.layers.Dense(6, activation=\"relu\")(base)\n",
    "        base = tf.keras.layers.Dense(1, activation=\"sigmoid\")(base)\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=[z,base])\n",
    "        return model\n",
    "    \n",
    "    def Hessian_DualLoss(InputLength):\n",
    "        #Inputs\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "        \n",
    "        #Baseline Importance of Each Function\n",
    "        add_branch = tf.keras.layers.Flatten()(inputA)\n",
    "        add_branch = tf.keras.layers.Dense(6, activation=\"relu\")(add_branch)\n",
    "        #add_branch = tf.keras.layers.BatchNormalization()(add_branch)\n",
    "        add_branch = tf.keras.Model(inputs=inputA, outputs=add_branch)\n",
    "        \n",
    "        \n",
    "        #Branch 2 - Interactions\n",
    "        hessian = tf.keras.layers.Flatten()(inputA)\n",
    "        hessian = tf.keras.layers.Dense(14*14, activation=\"relu\")(hessian)\n",
    "        hessian = tf.keras.layers.Reshape((-1, 14, 14))(hessian)\n",
    "        y = tf.keras.layers.Dropout(0.2)(hessian)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2D(2, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Conv2D(14, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.GlobalMaxPooling2D()(y)\n",
    "        y = tf.keras.layers.Dense(6, activation=\"relu\")(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.Model(inputs=inputA, outputs=y)\n",
    "\n",
    "        #combine the output of the two branches - add branch gives main weight, multiply gives interactions\n",
    "        #combined = tf.keras.layers.add([add_branch.output,tf.keras.layers.multiply([x.output, y.output])])\n",
    "        combined = tf.keras.layers.concatenate([add_branch.output,y.output])\n",
    "\n",
    "        # Combined Outputs\n",
    "        z = tf.keras.layers.Flatten()(combined)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.BatchNormalization()(z)\n",
    "        z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "        \n",
    "        base = tf.keras.layers.Flatten()(add_branch.output)\n",
    "        base = tf.keras.layers.Dense(1, activation=\"sigmoid\")(base)\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=[z,base])\n",
    "        return model\n",
    "    \n",
    "    def Hessian_Straight_Model(InputLength):\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "\n",
    "        y = tf.keras.layers.Flatten()(inputA)\n",
    "        y = tf.keras.layers.Dense(14*14)(y)\n",
    "        y = tf.keras.layers.Reshape((-1, 14, 14))(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2D(2, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Conv2D(14, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.GlobalMaxPooling2D()(y)\n",
    "        y = tf.keras.layers.Dense(14, activation=\"relu\")(y)\n",
    "        y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dense(6, activation=\"relu\")(y)\n",
    "        y = tf.keras.layers.Dense(1, activation=\"sigmoid\")(y)\n",
    "\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=y)\n",
    "        return model\n",
    "        return model\n",
    "\n",
    "    def Hessian_Big_model (InputLength):\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "        hessian = tf.keras.layers.Flatten()(inputA)\n",
    "        hessian = tf.keras.layers.Dense(14*14, activation=\"relu\")(hessian)\n",
    "        #hessian = tf.keras.layers.Reshape((-1, 14, 14))(hessian)\n",
    "\n",
    "        # Dense Input Branch\n",
    "        #x = tf.keras.layers.Flatten()(hessian)\n",
    "        x = tf.keras.layers.Dense(6, activation=\"relu\")(hessian)\n",
    "        x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "\n",
    "        y = tf.keras.layers.Reshape((-1, 14, 14))(hessian)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2D(2, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Conv2D(14, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.GlobalMaxPooling2D()(y)\n",
    "        y = tf.keras.layers.Dense(6, activation=\"relu\")(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.Model(inputs=inputA, outputs=y)\n",
    "\n",
    "        #combine the output of the two branches\n",
    "        combined = tf.keras.layers.multiply([x.output, y.output])\n",
    "\n",
    "        # Combined Outputs\n",
    "        z = tf.keras.layers.Flatten()(combined)\n",
    "        z = tf.keras.layers.Dense(14, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dropout(0.2)(z)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.BatchNormalization()(z)\n",
    "        z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=z)\n",
    "        return model\n",
    "    \n",
    "    def Hessian_Mixed_Model(InputLength):\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "\n",
    "        # Dense Input Branch\n",
    "        x = tf.keras.layers.Flatten()(inputA)\n",
    "        #x = tf.keras.layers.Dense(6)(x)\n",
    "        x = tf.keras.layers.Dense(14, activation=\"relu\")(x)\n",
    "        #x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "\n",
    "        y = tf.keras.layers.Flatten()(inputA)\n",
    "        y = tf.keras.layers.Dense(6)(y)\n",
    "        y = tf.keras.layers.Dense(14*14)(y)\n",
    "        y = tf.keras.layers.Reshape((-1, 14, 14))(y)\n",
    "\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2D(2, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Conv2D(14, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.GlobalMaxPooling2D()(y)\n",
    "        y = tf.keras.layers.Dense(14, activation=\"relu\")(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.Model(inputs=inputA, outputs=y)\n",
    "\n",
    "        #combine the output of the two branches\n",
    "        combined = tf.keras.layers.multiply([x.output, y.output])\n",
    "\n",
    "        # Combined Outputs\n",
    "        z = tf.keras.layers.Flatten()(combined)\n",
    "        z = tf.keras.layers.Dense(14, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.BatchNormalization()(z)\n",
    "        z = tf.keras.layers.Dropout(0.2)(z)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=z)\n",
    "        return model\n",
    "\n",
    "\n",
    "    def genNN_mixed_model(InputLength):# define two sets of inputs\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "\n",
    "        # Dense Input Branch\n",
    "        x = tf.keras.layers.Flatten()(inputA)\n",
    "        x = tf.keras.layers.Dense(14, activation=\"relu\")(x)\n",
    "        #x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "        \n",
    "        #inputB = tf.reshape(inputA,(-1,14,InputLength,1))\n",
    "        # the second branch opreates on the second input\n",
    "        y = tf.keras.layers.Reshape((-1, -1, 1))(inputA)\n",
    "        y = tf.keras.layers.Conv2D(16, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Conv2D(32, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Reshape((14, InputLength))(inputA)\n",
    "        y = tf.keras.layers.Flatten()(y)\n",
    "        y = tf.keras.layers.Dense(14, activation=\"relu\")(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.Model(inputs=inputA, outputs=y)\n",
    "        \n",
    "        #combine the output of the two branches\n",
    "        combined = tf.keras.layers.add([x.output, y.output])\n",
    "        #combined = tf.keras.layers.concatenate([x.output, y.output])\n",
    "\n",
    "        # Combined Outputs\n",
    "        z = tf.keras.layers.Flatten()(combined)\n",
    "        z = tf.keras.layers.Dense(14, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dropout(0.2)(z)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=z)\n",
    "        return model\n",
    "    \n",
    "    def Hessian_Mixed_AddMult(InputLength):\n",
    "        #Inputs\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "        \n",
    "        #Baseline Importance of Each Function\n",
    "        add_branch = tf.keras.layers.Flatten()(inputA)\n",
    "        add_branch = tf.keras.layers.Dense(6, activation=\"relu\")(add_branch)\n",
    "        #add_branch = tf.keras.layers.BatchNormalization()(add_branch)\n",
    "        add_branch = tf.keras.Model(inputs=inputA, outputs=add_branch)\n",
    "        \n",
    "        # Dense Input Branch - How important is each position\n",
    "        x = tf.keras.layers.Flatten()(inputA)\n",
    "        x = tf.keras.layers.Dense(6, activation=\"relu\")(x)\n",
    "        x = tf.keras.Model(inputs=inputA, outputs=x)\n",
    "        \n",
    "        #Branch 2 - Interactions\n",
    "        hessian = tf.keras.layers.Flatten()(inputA)\n",
    "        hessian = tf.keras.layers.Dense(14*14, activation=\"relu\")(hessian)\n",
    "        hessian = tf.keras.layers.Reshape((-1, 14, 14))(hessian)\n",
    "        y = tf.keras.layers.Dropout(0.2)(hessian)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        y = tf.keras.layers.Conv2DTranspose(2, (5, 5), strides=(2, 2), padding='same', use_bias=False)(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.Conv2D(2, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        y = tf.keras.layers.Conv2D(14, (5, 5), strides=(2, 2), padding='same')(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.layers.Dropout(0.2)(y)\n",
    "        y = tf.keras.layers.GlobalMaxPooling2D()(y)\n",
    "        y = tf.keras.layers.Dense(6, activation=\"relu\")(y)\n",
    "        #y = tf.keras.layers.BatchNormalization()(y)\n",
    "        y = tf.keras.Model(inputs=inputA, outputs=y)\n",
    "\n",
    "        #combine the output of the two branches - add branch gives main weight, multiply gives interactions\n",
    "        combined = tf.keras.layers.add([add_branch.output,tf.keras.layers.multiply([x.output, y.output])])\n",
    "\n",
    "        # Combined Outputs\n",
    "        z = tf.keras.layers.Flatten()(combined)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.Dropout(0.2)(z)\n",
    "        z = tf.keras.layers.Dense(6, activation=\"relu\")(z)\n",
    "        z = tf.keras.layers.BatchNormalization()(z)\n",
    "        z = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "\n",
    "        #Combined model outputs join of branches\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=z)\n",
    "        return model\n",
    "    \n",
    "    def basicModel(InputLength):\n",
    "        #Inputs\n",
    "        inputA = tf.keras.Input(shape=(14,InputLength))\n",
    "\n",
    "        x = tf.keras.layers.Flatten()(inputA)\n",
    "        x = tf.keras.layers.Dense(6, activation=\"relu\")(x) #Primary Importance of Each Basis Function\n",
    "        x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=[inputA], outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def genNNModelLim(InputLength):\n",
    "        tf.keras.backend.set_floatx('float64')\n",
    "        model = tf.keras.models.Sequential([\n",
    "          tf.keras.layers.Flatten(input_shape=(14, InputLength)),\n",
    "          tf.keras.layers.Dense(14, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.1),\n",
    "          #tf.keras.layers.Dense(32, activation='relu'),\n",
    "          #tf.keras.layers.Dropout(0.1),\n",
    "          tf.keras.layers.Dense(14, activation='relu'),\n",
    "          tf.keras.layers.Dropout(0.1),\n",
    "          #tf.keras.layers.Dense(64, activation='relu'),\n",
    "          tf.keras.layers.Dense(14, activation='relu'),\n",
    "          tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def DualLoss(y_true,output):\n",
    "        return tf.reduce_mean(tf.square(tf.add(tf.sub(y,y_),tf.sub(y,y_))))\n",
    "    \n",
    "    def NNOptimizer(lr=0.001,epsilon=1e-07,amsgrad=False):\n",
    "        return tf.keras.optimizers.Adam(learning_rate=lr, epsilon=epsilon, amsgrad=amsgrad, name='Adam')\n",
    "    \n",
    "    def cum_KL_divergence(ge_c,ge_mean,ge_sigma):\n",
    "        def KL_div(cdf_pred, cdf_true):\n",
    "            P = NN.TF_pdf(NN.TF_ppf(cdf_pred,ge_c,ge_mean,ge_sigma),ge_c,ge_mean,ge_sigma) #CDF -> PDF\n",
    "            Q = NN.TF_pdf(NN.TF_ppf(cdf_true,ge_c,ge_mean,ge_sigma),ge_c,ge_mean,ge_sigma)  #CDF -> PDF\n",
    "            P = tf.where(tf.math.is_nan(P),tf.convert_to_tensor(0.000001,dtype=tf.float64),P)\n",
    "            Q = tf.where(tf.math.is_nan(Q),tf.convert_to_tensor(0.000001,dtype=tf.float64),Q)\n",
    "\n",
    "            return tf.math.square(cdf_pred-cdf_true)#+P*tf.math.log(P/Q)\n",
    "        return KL_div\n",
    "\n",
    "    \n",
    "    def TF_ppf(q,c,mean,sigma):\n",
    "        x=-tf.math.log(-tf.math.log(q))\n",
    "        \n",
    "        return tf.where((c == 0) & (x == x),x,-tf.math.expm1(-c *x )/c)*sigma+mean\n",
    "    \n",
    "    def TF_pdf(x,c,mean,sigma):\n",
    "        z = (x-mean)/sigma\n",
    "        return tf.where((c==0),tf.math.exp(z)*tf.math.exp(-1*tf.math.exp(z)),tf.math.exp(-(1-c*z)**(1/c))*(1-c*z)**(1/c-1))*(1/sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwimSearch:\n",
    "    def seededList(df):\n",
    "        alphabet = SwimDNA.DNABasis()\n",
    "        seedlist=  np.array([])\n",
    "        for a in alphabet[1:-1]:\n",
    "            seedlist = np.append(seedlist,(a*6))\n",
    "            seedlist = np.append(seedlist,a+alphabet[0]*5)\n",
    "            seedlist = np.append(seedlist,a+alphabet[-1]*5)\n",
    "        \n",
    "        seed = pd.DataFrame({'DNA':seedlist})\n",
    "        seed = df[df.DNA.isin(seed.DNA)]\n",
    "        \n",
    "        return seed\n",
    "    \n",
    "    def normalizeArray(array,minV,maxV):\n",
    "        return (array-minV)/float(maxV-minV)\n",
    "\n",
    "    def rescaleArray(array,minV,maxV):\n",
    "        return (array)*float(maxV-minV)+minV\n",
    "\n",
    "    def top_predicted(dataframe,num=2000):\n",
    "        topLabel = dataframe.nlargest(num,\"Label\")\n",
    "        topModel = dataframe.nlargest(num,\"ModelLabel\")\n",
    "        correct = topModel[topModel.DNA.isin(topLabel.DNA)==True].DNA.size\n",
    "        print(f'correct: {correct}')\n",
    "        return float(correct)/float(len(topLabel.DNA.to_list()))\n",
    "    \n",
    "    def SearchMethodHandler(searchmethod):\n",
    "        #Handles the swimmer selection algorithim for the next generation, defaults to random\n",
    "        #Terms include 'Method1', 'Random'\n",
    "        if searchmethod == 'Method1':\n",
    "            print('Search Method: Method1 - 1/2 Top Predicted, 1/2 Random')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_Method1\n",
    "            \n",
    "        elif searchmethod == 'Method2':\n",
    "            print('Search Method: Vector Distance Directed Evolution')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_Method2\n",
    "        \n",
    "        elif searchmethod == 'MinAndMax':\n",
    "            print('Search Method: Top 1/2. Bottom 1/2')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_MinAndMax\n",
    "        \n",
    "        elif searchmethod == 'MinMaxDE':\n",
    "            print('Search Method: Top 1/2. Bottom 1/2, wih directed evolution')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_MinMaxDE\n",
    "        \n",
    "        elif searchmethod == 'Random':\n",
    "            print('Search Method: Random')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_Random\n",
    "        \n",
    "        \n",
    "        elif searchmethod == 'Max':\n",
    "            print('Search Method: Max')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_Max\n",
    "        \n",
    "        elif searchmethod == 'MaxDE':\n",
    "            print('Search Method: Top 1/2. directed evolution bottom half')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_MaxDE\n",
    "        \n",
    "        else:\n",
    "            print('Search Method: no method picked. Defaulting to random.')\n",
    "            searchfunc = SwimSearch.Next_Gen_Train_Random\n",
    "\n",
    "        return searchfunc\n",
    "\n",
    "    \n",
    "    def Next_Gen_Train_Method1(df,train,simsperloop,loopnum,seed=1337):\n",
    "        #Training swimmers are seeded with 1/2 random, and 1/2 top predicted swimmers by the model\n",
    "        i = 0\n",
    "        check = True\n",
    "        while check:\n",
    "            top = df.nlargest(int(np.round(simsperloop/2.)+i),'ModelLabel')\n",
    "            includeNum = top[~top.DNA.isin(train.DNA)].DNA.size\n",
    "            if (includeNum>=int(np.round(simsperloop/2.))):\n",
    "                check = False\n",
    "            elif (i>1000):\n",
    "                check = False\n",
    "            i+=1\n",
    "        print(top.DNA.tolist())\n",
    "\n",
    "\n",
    "        #Append Training Set with top Untrained Values and some random states\n",
    "        remain = simsperloop-top[~top.DNA.isin(train.DNA)].DNA.size #Take random samples for the remaining simulaitons\n",
    "        print(f'remaining: {remain}')\n",
    "        #print(f'TopSize: {top[~top.DNA.isin(train.DNA)].size}')\n",
    "\n",
    "        train = train.append(top[~top.DNA.isin(train.DNA)])\n",
    "        train = train.append(df[~df.DNA.isin(train.DNA)].sample(remain,random_state=seed+simsperloop*loopnum+seed*simsperloop)) \n",
    "\n",
    "        return train\n",
    "    \n",
    "    def Next_Gen_Train_Method2(df,train,simsperloop,loopnum,seed=1337):\n",
    "        top = df[~df.DNA.isin(train.DNA)].nlargest(int(np.round(simsperloop)),'ModelLabel')\n",
    "        top['AvgDistance'] =  SwimDNA.avgDnaListDistance(top.DNA)\n",
    "        \n",
    "        mid = top.nlargest(1,'AvgDistance').DNA.to_list()[0]\n",
    "        far = top.nsmallest(1,'AvgDistance').DNA.to_list()[0]\n",
    "        \n",
    "        DNAlength = len(train.DNA.to_list()[0])\n",
    "        \n",
    "        GenNum = loopnum\n",
    "        SwimNum = int(np.round(simsperloop/2))-1\n",
    "        middnalist = GeoSwimmer.GeoSwimmer.GenerateSwimArray(mid,SwimNum,GenNum,DNAlength,EvolveDNA=True,printDATA=False,saveDATA = '')\n",
    "        fardnalist = GeoSwimmer.GeoSwimmer.GenerateSwimArray(far,SwimNum,GenNum,DNAlength,EvolveDNA=True,printDATA=False,saveDATA = '')\n",
    "        \n",
    "        #Combine into a DNA list\n",
    "        dnalist = np.append(middnalist,fardnalist)\n",
    "        dnalist = np.append(dnalist,far)\n",
    "        dnalist = np.append(dnalist,mid)\n",
    "        \n",
    "        #Lookup values from full dataframe\n",
    "        top = pd.DataFrame({'DNA':dnalist})\n",
    "        top = df[df.DNA.isin(top.DNA)]\n",
    "\n",
    "        #Append Training Set with top Untrained Values and some random states\n",
    "        remain = simsperloop-top[~top.DNA.isin(train.DNA)].DNA.size #Take random samples for the remaining simulaitons\n",
    "        print(f'remaining: {remain}')\n",
    "\n",
    "        train = train.append(top[~top.DNA.isin(train.DNA)])\n",
    "        #Append Training Set with random states if there is overlap\n",
    "        train = train.append(df[~df.DNA.isin(train.DNA)].sample(remain,random_state=seed+simsperloop*loopnum+seed*simsperloop)) \n",
    "\n",
    "        return train\n",
    "    \n",
    "    def Next_Gen_Train_MinMaxDE(df,train,simsperloop,loopnum,seed=1337):\n",
    "        top = df[~df.DNA.isin(train.DNA)].nlargest(int(np.round(simsperloop)),'ModelLabel')\n",
    "        top['AvgDistance'] =  SwimDNA.avgDnaListDistance(top.DNA)\n",
    "        \n",
    "        bottom = df[~df.DNA.isin(train.DNA)].nsmallest(int(np.round(simsperloop)),'ModelLabel')\n",
    "        bottom['AvgDistance'] =  SwimDNA.avgDnaListDistance(bottom.DNA)\n",
    "        \n",
    "        top_mid = top.nlargest(1,'AvgDistance').DNA.to_list()[0]\n",
    "        top_far = top.nsmallest(1,'AvgDistance').DNA.to_list()[0]\n",
    "        \n",
    "        bot_mid = bottom.nlargest(1,'AvgDistance').DNA.to_list()[0]\n",
    "        bot_far = bottom.nsmallest(1,'AvgDistance').DNA.to_list()[0]\n",
    "        \n",
    "        DNAlength = len(train.DNA.to_list()[0])\n",
    "        \n",
    "        GenNum = loopnum\n",
    "        SwimNum = int(np.round(simsperloop/4))-1\n",
    "        top_mid_DNA = GeoSwimmer.GeoSwimmer.GenerateSwimArray(top_mid,SwimNum,GenNum,DNAlength,EvolveDNA=True,printDATA=False,saveDATA = '')\n",
    "        top_far_DNA = GeoSwimmer.GeoSwimmer.GenerateSwimArray(top_far,SwimNum,GenNum,DNAlength,EvolveDNA=True,printDATA=False,saveDATA = '')\n",
    "        \n",
    "        bottom_mid_DNA = GeoSwimmer.GeoSwimmer.GenerateSwimArray(bot_mid,SwimNum,GenNum,DNAlength,EvolveDNA=True,printDATA=False,saveDATA = '')\n",
    "        bottom_far_DNA = GeoSwimmer.GeoSwimmer.GenerateSwimArray(bot_far,SwimNum,GenNum,DNAlength,EvolveDNA=True,printDATA=False,saveDATA = '')\n",
    "        \n",
    "        \n",
    "        #Combine into a DNA list\n",
    "        dnalist = np.append(top_mid_DNA,top_far_DNA)\n",
    "        dnalist = np.append(dnalist,top_far)\n",
    "        dnalist = np.append(dnalist,top_mid)\n",
    "        \n",
    "        dnalist = np.append(dnalist,bottom_mid_DNA)\n",
    "        dnalist = np.append(dnalist,bottom_far_DNA)\n",
    "        dnalist = np.append(dnalist,bot_far)\n",
    "        dnalist = np.append(dnalist,bot_mid)\n",
    "        \n",
    "        #Lookup values from full dataframe\n",
    "        top = pd.DataFrame({'DNA':dnalist})\n",
    "        top = df[df.DNA.isin(top.DNA)]\n",
    "\n",
    "        #Append Training Set with top Untrained Values and some random states\n",
    "        remain = simsperloop-top[~top.DNA.isin(train.DNA)].DNA.size #Take random samples for the remaining simulaitons\n",
    "        print(f'remaining: {remain}')\n",
    "\n",
    "        train = train.append(top[~top.DNA.isin(train.DNA)])\n",
    "        #Append Training Set with random states if there is overlap/ already been studied\n",
    "        train = train.append(df[~df.DNA.isin(train.DNA)].sample(remain,random_state=seed+simsperloop*loopnum+seed*simsperloop)) \n",
    "\n",
    "        return train\n",
    "    \n",
    "    def Next_Gen_Train_MaxDE(df,train,simsperloop,loopnum,seed=1337):\n",
    "        top_half = df[~df.DNA.isin(train.DNA)].nlargest(int(np.round(simsperloop)/2),'ModelLabel').DNA.to_list()\n",
    "        top_DNA = df.nlargest(1,\"ModelLabel\").DNA.to_list()[0]\n",
    "\n",
    "        \n",
    "        DNAlength = len(train.DNA.to_list()[0])     \n",
    "        GenNum = loopnum\n",
    "        SwimNum = int(np.round(simsperloop/2))-1\n",
    "        top_DE = GeoSwimmer.GeoSwimmer.GenerateSwimArray(top_DNA,SwimNum,GenNum,DNAlength,EvolveDNA=True,printDATA=False,saveDATA = '')\n",
    "        \n",
    "        #Combine into a DNA list\n",
    "        dnalist = np.append(top_half,top_DE)\n",
    "        print(dnalist)\n",
    "\n",
    "        \n",
    "        #Lookup values from full dataframe\n",
    "        top = pd.DataFrame({'DNA':dnalist})\n",
    "        top = df[df.DNA.isin(top.DNA)]\n",
    "\n",
    "        #Append Training Set with top Untrained Values and some random states\n",
    "        remain = simsperloop-top[~top.DNA.isin(train.DNA)].DNA.size #Take random samples for the remaining simulaitons\n",
    "        print(f'remaining: {remain}')\n",
    "\n",
    "        train = train.append(top[~top.DNA.isin(train.DNA)])\n",
    "        #Append Training Set with random states if there is overlap/ already been studied\n",
    "        train = train.append(df[~df.DNA.isin(train.DNA)].sample(remain,random_state=seed+simsperloop*loopnum+seed*simsperloop)) \n",
    "\n",
    "        return train\n",
    "    \n",
    "    def Next_Gen_Train_MinAndMax(df,train,simsperloop,loopnum,seed=1337):\n",
    "        top = df[~df.DNA.isin(train.DNA)].nlargest(int(np.round(simsperloop/2.)),'ModelLabel')\n",
    "        bottom = df[~df.DNA.isin(train.DNA)].nsmallest(int(np.round(simsperloop/2.)),'ModelLabel')\n",
    "\n",
    "        train = train.append(top[~top.DNA.isin(train.DNA)])\n",
    "        train = train.append(bottom[~bottom.DNA.isin(train.DNA)])\n",
    "        \n",
    "        return train\n",
    "    \n",
    "    def Next_Gen_Train_Max(df,train,simsperloop,loopnum,seed=1337):\n",
    "        top = df[~df.DNA.isin(train.DNA)].nlargest(int(np.round(simsperloop)),'ModelLabel')\n",
    "        #bottom = df[~df.DNA.isin(train.DNA)].nsmallest(int(np.round(simsperloop/2.)),'ModelLabel')\n",
    "\n",
    "        train = train.append(top[~top.DNA.isin(train.DNA)])\n",
    "        #train = train.append(bottom[~bottom.DNA.isin(train.DNA)])\n",
    "        \n",
    "        return train\n",
    "    \n",
    "    def Next_Gen_Train_Random(df,train,simsperloop,loopnum,seed=1337):\n",
    "        #Training swimmers are seeded with totally random swimmers\n",
    "        top = df[~df.DNA.isin(train.DNA)].sample(simsperloop,random_state=seed+simsperloop*loopnum+seed*simsperloop)\n",
    "        train = train.append(top[~top.DNA.isin(train.DNA)])\n",
    "        print(f'NewSwimmers: {top.DNA.tolist()}')\n",
    "        \n",
    "        return train\n",
    "\n",
    "\n",
    "\n",
    "    def chunks(lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "    \n",
    "    def top_predicted_vel(dataframe,num=2000):\n",
    "        topLabel = dataframe.nlargest(num,\"Label\")\n",
    "        topModel = dataframe.nlargest(num,\"ModelLabel\")\n",
    "        correct = topModel[topModel.DNA.isin(topLabel.DNA)==True].DNA.size\n",
    "        print(f'correct: {correct}')\n",
    "        return float(correct)/float(len(topLabel.DNA.to_list()))\n",
    "    \n",
    "   \n",
    "    def Train_Loop_cdf(savepath,loadpath,model,breaksize=12, OH_df = np.array([]), seed=12345, InitialNumbers=1000,testNum=1000,epochsperloop=100,loops=10,simsperloop=100, save=False,method='Random',lr=0.001,epsilon=1e-07, dualloss=False):\n",
    "\n",
    "        df = pd.read_pickle(loadpath+'results.pkl')\n",
    "        df['ModelLabel'] = np.zeros(df.DNA.size)\n",
    "        dnaLength = len(df.DNA[0])\n",
    "        #print(f'DNAL: {dnaLength}')\n",
    "\n",
    "        #Initial Training Data/ Test Data Sampling\n",
    "        train = SwimSearch.seededList(df) #Seed with some selected basis functions (BAAAAA,BNNNNN,CAAAAA, etc)\n",
    "        remain = InitialNumbers-train.DNA.size\n",
    "        train = train.append(df[~df.DNA.isin(train.DNA)].sample(remain,random_state=seed))        \n",
    "        \n",
    "\n",
    "        #Initiate Neural Network Model\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        #Save Weights to Reset\n",
    "        model.save_weights(savepath+'model.h5')\n",
    "        \n",
    "        searchmethod = SwimSearch.SearchMethodHandler(method)\n",
    "        \n",
    "        if len(OH_df) == 0:\n",
    "            print('One Hot Encoding Dataframe...')\n",
    "            OH_df = NN.OneHotEncodeList(df)\n",
    "            print('Done.')\n",
    "        \n",
    "        if save:\n",
    "            TopPercentile = np.zeros(loops)\n",
    "            numcorrect = np.zeros(loops)\n",
    "\n",
    "        for loopnum in range(0,loops):\n",
    "            print(f'Train min Value: {train.Label.min()}')\n",
    "            print(f'Train Max Value: {train.Label.max()}')\n",
    "\n",
    "            #Reset Model - Reloading Weights\n",
    "            #model.load_weights(savepath+'model.h5')\n",
    "\n",
    "            #Estimate CDF of Training Samples\n",
    "            train_vels = np.array(train.Label.to_list()) #Convert of Array\n",
    "            print('Fitting CDF to array')\n",
    "            ge_c,ge_mean,ge_sigma = scipy.stats.genextreme.fit(train_vels) #Fit using extreme value thereom\n",
    "            print(f'ge_c: {ge_c}, ge_mean: {ge_mean}, ge_sigma: {ge_sigma}')\n",
    "            \n",
    "            \n",
    "            #Prepare Samples for Model\n",
    "            test = df[~df.DNA.isin(train.DNA)].sample(testNum,random_state=seed+1+loopnum*simsperloop) #Generate Test Set of Swimmers\n",
    "            train['CDF'] = scipy.stats.genextreme.cdf(train.Label, ge_c,loc=ge_mean,scale=ge_sigma)\n",
    "            test['CDF'] = scipy.stats.genextreme.cdf(test.Label, ge_c,loc=ge_mean,scale=ge_sigma)\n",
    "\n",
    "            OH_train = NN.OneHotEncodeList(train) #One Hot encoded tensor\n",
    "            OH_test = NN.OneHotEncodeList(test) #One Hot encoded tensor\n",
    "\n",
    "            train_label = tf.convert_to_tensor(train.CDF.to_numpy(),dtype=tf.float64)\n",
    "            test_label = tf.convert_to_tensor(test.CDF.to_numpy(),dtype=tf.float64)\n",
    "            \n",
    "\n",
    "            #Update Model\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            \n",
    "            if dualloss == False:\n",
    "                #Single Loss\n",
    "                model.compile(optimizer=NN.NNOptimizer(lr=lr,epsilon=epsilon),\n",
    "                          loss=mse,\n",
    "                          metrics=['accuracy'])\n",
    "                \n",
    "                #Fit Model to data\n",
    "                model.fit(OH_train, train_label, epochs=epochsperloop)\n",
    "                print(f'Loop {loopnum} MSE: {np.average((test.CDF.to_numpy()-model(OH_test,training=False).numpy())**2)}')\n",
    "            \n",
    "            else:\n",
    "                #Dual Loss\n",
    "                model.compile(optimizer=NN.NNOptimizer(lr=lr,epsilon=epsilon),\n",
    "                       loss=(mse,mse),\n",
    "                       metrics=['accuracy'])\n",
    "                \n",
    "                #Fit Model to data\n",
    "                model.fit(OH_train, (train_label,train_label), epochs=epochsperloop) #Multiloss\n",
    "                print(f'Loop {loopnum} MSE: {np.average((test.CDF.to_numpy()-model(OH_test,training=False)[0].numpy())**2)}') #Multiloss\n",
    "\n",
    "            \n",
    "            #Update Model Label\n",
    "            print('Updating Model Label...')\n",
    "            splits = int(np.round(df.DNA.size/breaksize))\n",
    "            breakList = list(SwimSearch.chunks(np.arange(df.DNA.size),splits))\n",
    "            modelLabel = np.zeros(df.DNA.size)\n",
    "            for breaks in breakList:\n",
    "                if dualloss == False :\n",
    "                    modelLabel[breaks[0]:breaks[-1]+1] = model(OH_df[breaks[0]:breaks[-1]+1],training=False).numpy().reshape(-1)\n",
    "                else:\n",
    "                    modelLabel[breaks[0]:breaks[-1]+1] = model(OH_df[breaks[0]:breaks[-1]+1],training=False)[0].numpy().reshape(-1) #Multiloss\n",
    "            df['ModelLabel']=modelLabel\n",
    "            \n",
    "\n",
    "\n",
    "            if save:\n",
    "                sample = df.sample(50000,random_state=seed)\n",
    "                OH_sample = NN.OneHotEncodeList(sample)\n",
    "                fig = RadarPlot.PlotRadarMesh(sample.RadX,sample.RadY,sample.ModelLabel,figwidth=9,figheight=6.5,vmin=0,vmax=1,res=100)\n",
    "                savefig(savepath+'Model_'+method+'_seed'+str(seed)+'_LoopNum'+str(loopnum)+'.png')\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                clf()\n",
    "                close()\n",
    "                \n",
    "                topnum = 2000\n",
    "                TopPercentile[loopnum] = SwimSearch.top_predicted_vel(df,topnum)\n",
    "                \n",
    "                topModel = df.nlargest(topnum,\"ModelLabel\")\n",
    "                numcorrect[loopnum] = topModel[topModel.DNA.isin(train.DNA)==True].DNA.size\n",
    "                #correct = topModel[topModel.DNA.isin(topLabel.DNA)==True].DNA.size\n",
    "        \n",
    "            #Update training data to include next results\n",
    "            train = searchmethod(df,train,simsperloop,loopnum,seed)\n",
    "        \n",
    "        \n",
    "        if save:\n",
    "            np.savetxt(savepath+'Loss1_'+method+'_seed'+str(seed)+'.txt',TopPercentile)\n",
    "            np.savetxt(savepath+'TopPerc2000_'+method+'_seed'+str(seed)+'.txt',TopPercentile)\n",
    "            np.savetxt(savepath+'CorSimulated2000_'+method+'_seed'+str(seed)+'.txt',numcorrect)\n",
    "        #return train\n",
    "        return model, df, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadpath = 'G:\\\\...\\\\'\n",
    "#e.g. 'G:\\\\Data Process\\\\NeuralNet\\\\EvaluationMethods\\\\'\n",
    "\n",
    "loadcheck = os.path.exists(loadpath+'results.pkl')\n",
    "if loadcheck == True:\n",
    "    df = pd.read_pickle(loadpath+'results.pkl')\n",
    "    print('Results loaded')\n",
    "else:\n",
    "    df = GenFooDataframe(6)\n",
    "    df.to_pickle(loadpath+'results.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('One Hot Encoding Dataframe...')\n",
    "OH_df = NN.OneHotEncodeList(df)\n",
    "print('Done.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automate all methods\n",
    "\n",
    "InitialtrainNum = 400\n",
    "breaksize = 32\n",
    "loops = 8+1 #Plus 1 to hit full Amount of 800 Swimmers\n",
    "simsperloop = 50\n",
    "epochsperloop = 100 #400#50\n",
    "save = True\n",
    "\n",
    "lr,epsilon = 0.001,1e-07\n",
    "\n",
    "\n",
    "methodslist = np.array([\"Method1\",\"Method2\",\"Random\",\"MinAndMax\",\"MinMaxDE\",\"MaxDE\",\"Max\"])\n",
    "seedlist = [1009,1013,1019,1021,1031,1033,1039,1049,1051,1061] #First 10 prime numbers with 4 digits\n",
    "\n",
    "dnaLength = len(df.DNA[0])\n",
    "\n",
    "\n",
    "#Pick which model to evaluate\n",
    "#model = NN.genNNModel(dnaLength)\n",
    "model = NN.basicModel(dnaLength)\n",
    "#model = NN.Hessian_DualLoss_refine(dnaLength)\n",
    "#model = NN.Hessian_DualLoss(dnaLength)\n",
    "#model = NN.Hessian_Mixed_Model(dnaLength)\n",
    "#model = NN.Hessian_Mixed_AddMult(dnaLength)\n",
    "#model = NN.Hessian_Big_model(dnaLength)\n",
    "#model = NN.Hessian_Straight_Model(dnaLength)\n",
    "#model = NN.genNN_mixed_model(dnaLength)\n",
    "#model = NN.genNN_MatMul_model(dnaLength)\n",
    "#model = NN.genNNModel(dnaLength)\n",
    "\n",
    "#Save Weights to Reset\n",
    "model.save_weights(loadpath+'model.h5')\n",
    "\n",
    "for method in methodslist:\n",
    "    print(method)\n",
    "    #Run An Example Training Model\n",
    "\n",
    "\n",
    "    searchmethod = method #Terms include 'Method1', 'Random', 'MaxDE', 'Max'\n",
    "    \n",
    "    savepath = loadpath+method+\"\\\\\"\n",
    "    \n",
    "\n",
    "    #print(savepath)\n",
    "    if (os.path.exists(savepath) == False):\n",
    "        os.mkdir(savepath[:-1])\n",
    "    \n",
    "    \n",
    "    for seed in seedlist:\n",
    "        model.load_weights(loadpath+'model.h5') \n",
    "        model, df,train = SwimSearch.Train_Loop_cdf(savepath,loadpath,model, breaksize=breaksize,OH_df = OH_df,seed=seed, InitialNumbers=InitialtrainNum,testNum=1000,epochsperloop=epochsperloop,loops=loops,simsperloop=simsperloop,save=save,method=searchmethod,lr=lr,epsilon=epsilon, dualloss=False)\n",
    "        \n",
    "        maxTrain = train.nlargest(1,'Label').Label.to_list()[0]\n",
    "        maxList = df.nlargest(1,'Label').Label.to_list()[0]\n",
    "        print(f'Label Predicted - {maxTrain}/{maxList}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_LBM",
   "language": "python",
   "name": "tf_lbm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
